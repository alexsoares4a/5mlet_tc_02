# ğŸ“Š Tech Challenge - Fase 2: Pipeline Batch Bovespa na AWS

Este repositÃ³rio contÃ©m a implementaÃ§Ã£o do projeto "Tech Challenge" da Fase 2 da PÃ³s-GraduaÃ§Ã£o em Machine Learning Engineering, focado na construÃ§Ã£o de um pipeline de dados batch robusto e automatizado para o mercado financeiro brasileiro.

O objetivo principal Ã© extrair, processar e analisar dados do pregÃ£o da B3 (Bolsa de Valores do Brasil) utilizando uma combinaÃ§Ã£o estratÃ©gica de serviÃ§os da AWS (S3, Glue, Lambda, Athena) e automatizando a ingestÃ£o de dados com GitHub Actions.

## âœ¨ Funcionalidades Principais

*   **Web Scraping Automatizado:** Coleta diÃ¡ria de dados do pregÃ£o da B3, orquestrada por **GitHub Actions**.
*   **Armazenamento em Camadas:** Dados brutos (`raw/`) e refinados (`refined/`) persistidos no Amazon S3, otimizados com particionamento por data.
*   **ETL Visual com AWS Glue:** Processamento, limpeza e transformaÃ§Ã£o dos dados utilizando o AWS Glue Studio, com o Glue Data Catalog para gerenciamento de metadados.
*   **OrquestraÃ§Ã£o Orientada a Eventos:** O pipeline de transformaÃ§Ã£o Ã© disparado automaticamente por eventos no S3, coordenados por AWS Lambda e Amazon EventBridge, garantindo a atualizaÃ§Ã£o do Glue Data Catalog antes do processamento.
*   **AnÃ¡lise Interativa:** Dados refinados disponÃ­veis para consulta e anÃ¡lise via SQL no Amazon Athena.
*   **GovernanÃ§a e SeguranÃ§a:** UtilizaÃ§Ã£o de variÃ¡veis de ambiente e GitHub Secrets para credenciais AWS, seguindo boas prÃ¡ticas de seguranÃ§a.

## ğŸ—ï¸ Arquitetura do Pipeline

O pipeline segue uma arquitetura em camadas, totalmente automatizada, desde a coleta dos dados atÃ© a disponibilizaÃ§Ã£o para anÃ¡lise:

[GITHUB ACTIONS]
     â†“ (diariamente Ã s 20h UTC)
[Web Scraping â†’ S3 raw/date=YYYY-MM-DD/]
     â†“ (S3 Event)
[Lambda 1 â†’ Start Glue Crawler]
     â†“
[Glue Crawler â†’ Glue Data Catalog]
     â†“ (Success Event)
[EventBridge â†’ Lambda 2]
     â†“
[Glue Job (Visual ETL)]
     â†“
[S3 refined/date_ingestao=/ticker=/]
     â†“
[Glue Data Catalog â†’ Athena]


### Detalhamento do Fluxo:

1.  Um workflow no GitHub Actions executa o script Python de web scraping (`scraping_b3.py`) diariamente.
2.  O script coleta os dados da B3 e os envia diretamente para a pasta `raw/` no S3, em formato Parquet e particionados por data.
3.  O upload de novos dados no S3 dispara uma AWS Lambda (`ibovespa-start-crawler-lambda`), que inicia o Glue Crawler.
4.  O Glue Crawler (`bovespa_raw_crawler`) varre a pasta `raw/`, detecta novas partiÃ§Ãµes e atualiza o Glue Data Catalog.
5.  ApÃ³s o sucesso do Crawler, o Amazon EventBridge (monitorando eventos do Glue) dispara outra AWS Lambda (`ibovespa-lambda-trigger`).
6.  Esta Lambda inicia o AWS Glue Job (`ibovespa-glue-job`), que processa os dados brutos com as transformaÃ§Ãµes necessÃ¡rias (agrupamento, renomeaÃ§Ã£o de colunas, cÃ¡lculo com datas).
7.  Os dados transformados sÃ£o salvos na pasta `refined/` no S3, tambÃ©m em Parquet e particionados por data e `ticker`.
8.  O Glue Job automaticamente atualiza o Glue Data Catalog com os metadados dos dados refinados.
9.  Por fim, os dados refinados ficam acessÃ­veis para consultas SQL interativas via Amazon Athena, utilizando o Glue Data Catalog.

## âš™ï¸ Tecnologias Utilizadas

*   **AutomaÃ§Ã£o/CI/CD:** [GitHub Actions](https://docs.github.com/en/actions)
*   **Web Scraping:** [Python](https://www.python.org/) com [Playwright](https://playwright.dev/) e [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
*   **ManipulaÃ§Ã£o de Dados:** [Pandas](https://pandas.pydata.org/)
*   **Armazenamento de Dados:** [Amazon S3](https://aws.amazon.com/s3/)
*   **ETL & CatÃ¡logo de Dados:** [AWS Glue](https://aws.amazon.com/glue/)
*   **OrquestraÃ§Ã£o de Eventos:** [AWS Lambda](https://aws.amazon.com/lambda/) e [Amazon EventBridge](https://aws.amazon.com/eventbridge/)
*   **Consulta SQL:** [Amazon Athena](https://aws.amazon.com/athena/)
*   **SDK AWS para Python:** [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

## ğŸš€ Como Executar o Projeto

Para replicar e executar este pipeline, siga os passos abaixo:

### PrÃ©-requisitos

*   Conta AWS ativa.
*   Um bucket S3 criado (ex: `ibovespa-bucket-data`) com pastas `raw/` e `refined/`.
*   Credenciais AWS (`Access Key ID` e `Secret Access Key`) com permissÃµes para:
    *   Escrever no bucket S3.
    *   Criar e gerenciar funÃ§Ãµes Lambda.
    *   Criar e gerenciar Glue Crawlers e Jobs.
    *   Acesso ao EventBridge.
*   RepositÃ³rio no GitHub.

### ğŸ” ConfiguraÃ§Ã£o das Credenciais AWS no GitHub Secrets

As credenciais AWS necessÃ¡rias para o script de scraping sÃ£o gerenciadas de forma segura atravÃ©s dos GitHub Secrets.

1.  No seu repositÃ³rio GitHub, navegue atÃ© `Settings > Secrets and variables > Actions`.
2.  Clique em `New repository secret` e adicione as seguintes secrets:
    *   `AWS_ACCESS_KEY_ID`: Sua AWS Access Key ID.
    *   `AWS_SECRET_ACCESS_KEY`: Sua AWS Secret Access Key.

### ğŸ“‚ Estrutura do Projeto

Clone o repositÃ³rio e observe a seguinte estrutura:

techchallenge-fase2/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ scraping_b3.py          # Script principal de scraping
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scraping-b3.yml     # Workflow do GitHub Actions
â”œâ”€â”€ requirements.txt            # DependÃªncias Python
â”œâ”€â”€ .env.example                # Modelo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md                   # Este arquivo


### ğŸƒ Executando o Pipeline

A etapa de web scraping e ingestÃ£o de dados brutos Ã© totalmente automatizada via GitHub Actions.

*   **Agendamento:** O workflow `scraping-b3.yml` estÃ¡ configurado para executar diariamente Ã s **8h da manhÃ£ (UTC)** automaticamente.
*   **Disparo Manual:** Para testar ou executar o scraping sob demanda, vocÃª pode disparar o workflow manualmente:
    1.  No seu repositÃ³rio GitHub, navegue atÃ© a aba `Actions`.
    2.  No menu lateral esquerdo, clique em `Scraping B3 - ExecuÃ§Ã£o DiÃ¡ria`.
    3.  No lado direito, clique no botÃ£o `Run workflow` e confirme.

O restante do pipeline (Lambda, EventBridge, Glue, Athena) Ã© orquestrado por eventos na AWS, sendo disparado automaticamente apÃ³s o sucesso da ingestÃ£o de dados brutos no S3.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.